{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "99a1ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load modules\n",
    "import numpy as np\n",
    "import RNA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from collections import Counter,defaultdict\n",
    "from Bio import SeqIO\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29603200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all functions\n",
    "\n",
    "## basic stats\n",
    "def seq_composition(s):\n",
    "    L = len(s)\n",
    "    nts_counts = Counter(s)\n",
    "    A = nts_counts['A']\n",
    "    C = nts_counts['C']\n",
    "    G = nts_counts['G']\n",
    "    U = nts_counts['U']\n",
    "    GC = (C+G)/L\n",
    "    return [A/L,C/L,G/L,U/L,\n",
    "            A/C,A/G,A/U, C/G,C/U, G/U,\n",
    "            GC]\n",
    "\n",
    "## homopolymers\n",
    "def find_homopolymer(seq):\n",
    "    L = len(seq)\n",
    "    homopolymers = [0,0,0,0]\n",
    "    i = 0\n",
    "    shortest_homopolymer_len = 4\n",
    "    \n",
    "    while i < L - shortest_homopolymer_len + 1:\n",
    "        base = seq[i]\n",
    "        homopolymer = base\n",
    "        for j in range(i+1,L):\n",
    "            if seq[j] == base:\n",
    "                homopolymer += base\n",
    "            else:\n",
    "                break\n",
    "        i = j\n",
    "        if len(homopolymer) >= shortest_homopolymer_len:\n",
    "            NT = list(set(homopolymer))[0]\n",
    "            if NT == 'A': homopolymers[0] += 1\n",
    "            elif NT == 'C': homopolymers[1] += 1\n",
    "            elif NT == 'G': homopolymers[2] += 1\n",
    "            elif NT == 'U': homopolymers[3] += 1\n",
    "    \n",
    "    return homopolymers\n",
    "\n",
    "## entropy + parity\n",
    "def shannon_entropy(kmer_counts):\n",
    "    kmer_counts = [x for x in kmer_counts if x != 0]\n",
    "    \n",
    "    total_kmers = sum(kmer_counts)\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for count in kmer_counts:\n",
    "        probability = count / total_kmers\n",
    "        entropy -= probability * math.log2(probability)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def revcomp(seq):\n",
    "    revcomp = ''\n",
    "    for i in seq[::-1]:\n",
    "        revcomp += {'A':'U','C':'G','G':'C','U':'A'}.get(i,i)\n",
    "    return revcomp\n",
    "\n",
    "def kmer_parity(seq):\n",
    "    L = len(seq)\n",
    "    results = []\n",
    "    results_entropy = []\n",
    "    for klen in range(1,10,2):\n",
    "        kmer_counts = defaultdict(list)\n",
    "        individual_kmer_counts = []\n",
    "        for i in range(0,len(seq)+1-klen):\n",
    "            kmer = seq[i:i+klen]\n",
    "            rc_kmer = revcomp(kmer)\n",
    "            canonical_kmer = min(kmer,rc_kmer)\n",
    "            if canonical_kmer not in kmer_counts:\n",
    "                kmer_counts[canonical_kmer] = [0,0]\n",
    "            if kmer == canonical_kmer:\n",
    "                kmer_counts[canonical_kmer][0] += 1\n",
    "            else:\n",
    "                kmer_counts[canonical_kmer][1] += 1\n",
    "        dna_diff = 0\n",
    "        for k,v in kmer_counts.items():\n",
    "            dna_diff += abs(v[0]-v[1])\n",
    "            individual_kmer_counts += v\n",
    "        results.append(1 - dna_diff/L)\n",
    "        results_entropy.append(shannon_entropy(individual_kmer_counts))\n",
    "    return results + results_entropy\n",
    "\n",
    "## primers\n",
    "universal_primers = ['AGAGUUUGAUCCUGGCUCAG','AGAGUUUGAUC[AC]UGGCUCAG','ACUGCUGC[GC][CU]CCCGUAGGAGUCU','GACUCCUACGGGAGGC[AU]GCAG','GUAUUACCGCGGCUGCUGG','GUGCCAGC[AC]GCCGCGGUAA','GGAUUAGAUACCCUGGUA','GGACUAC[ACG][GC]GGGUAUCUAAU','CCGUCAAUUCCUUU[AG]AGUUU','UAAAACU[CU]AAA[GU]GAAUUGACGGG','[CU]AACGAGCGCAACCC','GGGUUGCGCUCGUUG','GGUUACCUUGUUACGACUU','CGGUUACCUUGUUACGACUU']\n",
    "up_regexes = [re.compile(x) for x in universal_primers]\n",
    "\n",
    "def find_universal_primers(s):\n",
    "    matches = set()\n",
    "    for regex in up_regexes:\n",
    "        for match in regex.finditer(s):\n",
    "            matches.add(match[0])\n",
    "    return len(matches)\n",
    "\n",
    "## frequent kmers\n",
    "def proportion_kmers_present(seq, biomarker_kmers):\n",
    "    present = 0\n",
    "    for k in biomarker_kmers:\n",
    "        if k in seq:\n",
    "            present += 1\n",
    "    return present/len(biomarker_kmers)\n",
    "\n",
    "def find_biomarker_kmers(f, klen, min_freq_SILVA):\n",
    "    kmer_counts = defaultdict(int)\n",
    "    kmers_of_interest = set()\n",
    "    total_seqs = 0\n",
    "    \n",
    "    for h,i in tqdm(enumerate(SeqIO.parse(f,'fasta'))):\n",
    "        s = str(i.seq).upper().replace('T','U')\n",
    "        total_seqs += 1\n",
    "        kmers = set()\n",
    "        for i in range(0,len(s)+1-klen):\n",
    "            kmer = s[i:i+klen]\n",
    "            kmers.add(kmer)\n",
    "        for k in kmers:\n",
    "            kmer_counts[k] += 1\n",
    "    \n",
    "    for k,v in sorted(kmer_counts.items(),key=lambda x:x[1]):\n",
    "        if v/total_seqs > min_freq_SILVA:\n",
    "            kmers_of_interest.add(k)\n",
    "            \n",
    "    return kmers_of_interest\n",
    "\n",
    "## gapped k-mers\n",
    "def load_gapped_kmers(k=11):\n",
    "    # fetch kmers\n",
    "    fn = './results/k-'+str(k)+'-300000.csv' # TODO: allow for different total_seqs\n",
    "    kmers = dict()\n",
    "    with open(fn, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            kmers[i] = literal_eval(line.split(';')[0])\n",
    "    return kmers\n",
    "\n",
    "def make_kmer_regexes(kmers):\n",
    "    k = len(kmers[0])\n",
    "    kmer_regexes = []\n",
    "    for km in kmers:\n",
    "        re_str = ''\n",
    "        chars = [x[0] for x in km]\n",
    "        offsets = [x[1] for x in km]\n",
    "        rel_offs = [offsets[i] - offsets[i-1] for i in range(1,k)]\n",
    "        for i in range(k):\n",
    "            re_str += chars[i]\n",
    "            if i < k-1 and rel_offs[i] > 1:\n",
    "                re_str += '.{' + str(rel_offs[i]-1) + '}'\n",
    "        kmer_regexes.append(re.compile(re_str))\n",
    "    return kmer_regexes\n",
    "\n",
    "## secondary structure\n",
    "def get_helices(base_pairs):\n",
    "    if not base_pairs:\n",
    "        return []\n",
    "\n",
    "    helices = []\n",
    "    current_helix = [base_pairs[0]]\n",
    "\n",
    "    for i in range(1, len(base_pairs)):\n",
    "        prev = base_pairs[i - 1]\n",
    "        curr = base_pairs[i]\n",
    "\n",
    "        # Check if it's part of the same helix (i.e., consecutive base pairs)\n",
    "        if curr[0] == prev[0] + 1 and curr[1] == prev[1] - 1:\n",
    "            current_helix.append(curr)\n",
    "        else:\n",
    "            helices.append(current_helix)\n",
    "            current_helix = [curr]\n",
    "\n",
    "    helices.append(current_helix)\n",
    "    return helices\n",
    "\n",
    "def get_base_pairs(dot_bracket):\n",
    "    stack = []\n",
    "    pairs = []\n",
    "    for i, char in enumerate(dot_bracket):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            j = stack.pop()\n",
    "            pairs.append((j, i))\n",
    "    return sorted(pairs)\n",
    "\n",
    "def predict_mfe_helices(rna_sequence):\n",
    "    helix_length_threshold = 10\n",
    "    fc = RNA.fold_compound(rna_sequence)\n",
    "    structure, mfe = fc.mfe()\n",
    "    base_pairs = get_base_pairs(structure)\n",
    "    helices = get_helices(base_pairs)\n",
    "    \n",
    "    reported_helices = [len(helix) for helix in helices if len(helix) >= helix_length_threshold]\n",
    "    \n",
    "    return mfe, len(reported_helices)\n",
    "\n",
    "## fingerprint?\n",
    "nt_sequence = 'UACAUAAAAUAAUAGGCAGGAUAAUAGAGGCAGCGCGAACUGUGAAAAGGCAAUAGACUAAUAAGAUACAAAAUAAAUGCGGCGUAAUCCGUGCAGGUAUUGACUGAAGGAUGUAGGCUGACCCCGUAAAGUCCGGCUGG'\n",
    "loci_sequence = [13,51,54,55,56,109,151,160,243,244,246,282,323,344,346,347,355,356,357,362,364,368,389,397,405,499,505,509,515,517,519,520,521,522,527,528,530,532,533,536,565,566,571,581,676,695,704,715,725,727,732,781,787,788,790,791,792,795,801,802,815,820,864,865,885,889,891,892,899,900,901,908,909,911,914,915,919,920,922,924,925,926,936,944,956,958,959,960,972,984,1050,1052,1053,1054,1055,1057,1058,1073,1093,1095,1199,1221,1227,1237,1315,1316,1318,1319,1337,1338,1339,1341,1347,1348,1349,1373,1379,1382,1391,1392,1394,1395,1397,1399,1403,1405,1406,1418,1492,1493,1494,1495,1496,1501,1504,1505,1509,1512,1517,1526]\n",
    "def universally_conserved_nucleotide_fingerprint():\n",
    "    ref_seq_N = ''\n",
    "    \n",
    "    for i in range(1, 1501):\n",
    "        if i not in loci_sequence:\n",
    "            ref_seq_N += 'N'\n",
    "        else:\n",
    "            ref_seq_N += nt_sequence[loci_sequence.index(i)]\n",
    "    \n",
    "    clustered_nt_sequence = []\n",
    "    \n",
    "    for i in ref_seq_N.split('N'):\n",
    "        if i != '': clustered_nt_sequence.append(i)\n",
    "    \n",
    "    return clustered_nt_sequence\n",
    "\n",
    "def find_conserved_nucleotide_fingerprint(query_sequence, nt_sequence):\n",
    "    found_univ_conserved_nts = []\n",
    "    start = 0\n",
    "\n",
    "    for i in nt_sequence:\n",
    "        loci = query_sequence.find(i,start,len(query_sequence)+1)\n",
    "        if loci != -1:\n",
    "            found_univ_conserved_nts.append(start)\n",
    "            start = loci + 1 + len(i) # adjust subsequence to search for next conserved motive in based upon position of last found motif\n",
    "        else: continue\n",
    "    return len(found_univ_conserved_nts)/len(nt_sequence),found_univ_conserved_nts\n",
    "\n",
    "## training\n",
    "def pred_eval(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel().tolist()\n",
    "    return tn, fp, fn, tp, tn/(fp+tn), tp/(tp+fn)\n",
    "\n",
    "def train_test_LR(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return pred_eval(y_test, y_pred)\n",
    "\n",
    "## timer\n",
    "class print_time:\n",
    "    def __init__(self, desc):\n",
    "        self.desc = desc\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(self.desc)\n",
    "        self.t = time.time()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        print(f'{self.desc} took {time.time()-self.t:.02f}s')\n",
    "\n",
    "## mutate\n",
    "def mutate_dna(rna_string, num_mutations):\n",
    "    rna_list = list(rna_string)\n",
    "    rna_length = len(rna_list)\n",
    "    if num_mutations > rna_length:\n",
    "        raise ValueError(\"Number of mutations cannot exceed the length of the RNA string.\")\n",
    "\n",
    "    mutation_indices = random.sample(range(rna_length), num_mutations)\n",
    "\n",
    "    for index in mutation_indices:\n",
    "        current_base = rna_list[index]\n",
    "        valid_bases = ['A', 'U', 'C', 'G']\n",
    "        if current_base not in valid_bases:\n",
    "            continue\n",
    "        else:\n",
    "            valid_bases.remove(current_base) # Ensure the new base is different.\n",
    "            new_base = random.choice(valid_bases)\n",
    "            rna_list[index] = new_base\n",
    "\n",
    "    return \"\".join(rna_list)  # Convert back to a string\n",
    "\n",
    "## load\n",
    "def load_data(filename):\n",
    "    r = 0.1\n",
    "    gt = []\n",
    "    mut = []\n",
    "    random.seed(42)\n",
    "    for i in SeqIO.parse(filename,'fasta'):\n",
    "        s = str(i.seq).upper()\n",
    "        m = mutate_dna(s, int(r*len(s)))\n",
    "        gt.append(s)\n",
    "        mut.append(m)\n",
    "    return gt, mut\n",
    "\n",
    "def run_train(gt, mut, func=seq_composition):\n",
    "    X_gt = []\n",
    "    X_mut = []\n",
    "\n",
    "    with print_time('running'):\n",
    "        for i in range(len(gt)):\n",
    "            s = gt[i]\n",
    "            m = mut[i]\n",
    "            X_gt.append(func(s))\n",
    "            X_mut.append(func(m))\n",
    "    \n",
    "    y_gt = [1] * len(X_gt)\n",
    "    y_mut = [0] * len(X_mut)\n",
    "    y = np.array(y_gt + y_mut)\n",
    "    \n",
    "    if type(X_gt[0]) is list or type(X_gt[0]) is tuple:\n",
    "        outs = []\n",
    "        for i in range(len(X_gt[0])):\n",
    "            X_gt_i = [x[i] for x in X_gt]\n",
    "            X_mut_i = [x[i] for x in X_mut]\n",
    "            X = np.array(X_gt_i + X_mut_i).reshape(-1, 1)\n",
    "            outs.append(train_test_LR(X, y))\n",
    "        return outs            \n",
    "    else:\n",
    "        X = np.array(X_gt + X_mut).reshape(-1, 1)\n",
    "        return train_test_LR(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fb7ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'SILVA_138.2_SSURef_NR99_tax_silva_filtered.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "gt, mut = load_data(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "101c61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 39.99s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 56535, 0, 56423, 0.0, 1.0),\n",
       " (29790, 26745, 24566, 31857, 0.5269302202175643, 0.5646101767009908),\n",
       " (35670, 20865, 20741, 35682, 0.6309365879543646, 0.6324016801658898),\n",
       " (33750, 22785, 22551, 33872, 0.5969753250198991, 0.6003225634936108),\n",
       " (30117, 26418, 26649, 29774, 0.5327142478110904, 0.5276926076245503),\n",
       " (31159, 25376, 23960, 32463, 0.5511453082161493, 0.5753504776420963),\n",
       " (34916, 21619, 22513, 33910, 0.6175997169894756, 0.6009960477110399),\n",
       " (37287, 19248, 17339, 39084, 0.6595383390819846, 0.6926962408946706),\n",
       " (32598, 23937, 28582, 27841, 0.57659856725922, 0.49343352888006664),\n",
       " (36351, 20184, 22303, 34120, 0.6429822234014327, 0.6047179341757793),\n",
       " (31752, 24783, 25522, 30901, 0.5616343857787212, 0.5476667316519859)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train(gt, mut, seq_composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e67edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 175.63s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(34507, 22028, 23277, 33146, 0.6103652604581233, 0.5874554702869397),\n",
       " (32940, 23595, 26946, 29477, 0.5826479172194216, 0.5224287967672757),\n",
       " (31712, 24823, 21713, 34710, 0.5609268594675865, 0.6151746628148096),\n",
       " (30808, 25727, 26709, 29714, 0.5449367648359423, 0.5266292114917676),\n",
       " (35689, 20846, 19872, 36551, 0.6312726629521536, 0.6478032008223596)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_homopolymer_and_bias(s):\n",
    "    hps = find_homopolymer(s)\n",
    "    return hps + [hps[2] / sum(hps)]\n",
    "\n",
    "run_train(gt, mut, find_homopolymer_and_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad7206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 7122.57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(39403, 17132, 16249, 40174, 0.69696648094101, 0.7120146039735569),\n",
       " (38853, 17682, 16175, 40248, 0.687237994162908, 0.7133261258706556),\n",
       " (37988, 18547, 19502, 36921, 0.6719377376846202, 0.6543608103078532),\n",
       " (34678, 21857, 20365, 36058, 0.6133899354382241, 0.6390656292646616),\n",
       " (29732, 26803, 19702, 36721, 0.525904307066419, 0.6508161565319107),\n",
       " (40163, 16372, 24606, 31817, 0.7104094808525692, 0.5639012459458023),\n",
       " (39968, 16567, 24081, 32342, 0.7069602900857875, 0.5732059621076512),\n",
       " (38715, 17820, 26199, 30224, 0.6847970283894932, 0.5356680786204208),\n",
       " (34130, 22405, 32463, 23960, 0.6036968249756788, 0.4246495223579037),\n",
       " (0, 56535, 0, 56423, 0.0, 1.0)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this handles all 5 k values in one run\n",
    "run_train(gt, mut, kmer_parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6663bd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 85.99s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55685, 850, 6970, 49453, 0.9849650658883877, 0.8764688159084062)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train(gt, mut, find_universal_primers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08f5c187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "282395it [04:09, 1133.88it/s]\n"
     ]
    }
   ],
   "source": [
    "kmers_of_interest = find_biomarker_kmers(f, klen = 11, min_freq_SILVA = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b24c70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 637.06s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56059, 476, 4742, 51681, 0.9915804368974971, 0.9159562589724048)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train(gt, mut, lambda x: proportion_kmers_present(x, kmers_of_interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb7c979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers = load_gapped_kmers()\n",
    "kmer_regexes = make_kmer_regexes([kmers[i] for i in range(len(kmers))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2abeb0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 559.59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53436, 3099, 3688, 52735, 0.9451843990448395, 0.9346365843716214)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train(gt, mut, lambda x: sum([kr.search(x) != None for kr in kmer_regexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64f82b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "running took 765.51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(9, 10, 12, 9, 0.47368421052631576, 0.42857142857142855),\n",
       " (10, 9, 10, 11, 0.5263157894736842, 0.5238095238095238)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_train(gt[:100], mut[:100], predict_mfe_helices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
